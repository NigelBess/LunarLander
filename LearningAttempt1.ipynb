{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to collect data for training a DQN to land the lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import NeuralNetUtils as nnu\n",
    "importlib.reload(nnu)\n",
    "\n",
    "from NeuralNetUtils import NeuralNetwork, ReplayBuffer\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers specific to the lunar lander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FEATURES = 8 # x, ẋ, y, ẏ, θ, θ*, rightlegtouching, leftlegtouching\n",
    "ACTION_OPTIONS = 4 # do nothing, fire left, fire main, fire right\n",
    "TOTAL_COLUMNS = STATE_FEATURES * 2 + ACTION_OPTIONS + 1 # S + A + R + S = 2*S + A + 1\n",
    "\n",
    "\n",
    "#converts env state space into a more easily useable object\n",
    "class EnvironmentState:\n",
    "    def __init__(self,state):\n",
    "        coord = state[0]\n",
    "        self.x = coord[0]\n",
    "        self.y = coord[1]\n",
    "        self.x_dot = coord[2]\n",
    "        self.y_dot = coord[3]\n",
    "        self.theta = coord[4]\n",
    "        self.theta_dot = coord[5]\n",
    "        self.leftLegTouching = int(coord[6])\n",
    "        self.rightLegTouching = int(coord[7])\n",
    "        reward = 0\n",
    "        isDone = False\n",
    "        if len(state)>2:\n",
    "            reward = state[1]\n",
    "            isDone = state[2] or state[3]\n",
    "        self.reward = reward\n",
    "        self.isDone = isDone \n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\n",
    "            f\"(x, y): ({self.x}, {self.y})\",\n",
    "            f\"(ẋ, ẏ): ({self.x_dot}, {self.y_dot})\",\n",
    "            f\"θ: {self.theta}\",\n",
    "            f\"dθ/dt: {self.theta_dot}\",\n",
    "            f\"Legs touching (L,R): ({self.leftLegTouching}, {self.rightLegTouching})\",\n",
    "            f\" Reward: {self.reward}\",\n",
    "            f\" Done: {self.isDone}\",\n",
    "        ]\n",
    "        return \"\\n\".join(lines,)\n",
    "\n",
    "    def toTensor(self):\n",
    "        return nnu.convertToTensor(self.toArray().astype(np.float32))\n",
    "    \n",
    "    def toArray(self) -> np.ndarray:\n",
    "        return np.array(self.toList())\n",
    "    \n",
    "    def toList(self) -> List[float]:\n",
    "        return [self.x, self.y, self.x_dot, self.y_dot, self.theta, self.theta_dot, self.leftLegTouching, self.rightLegTouching]\n",
    "    \n",
    "# reads file to load a replay buffer\n",
    "def LoadReplayBuffer(filePath : str) -> ReplayBuffer:\n",
    "\n",
    "    start_index = 0\n",
    "    def getNextColumns(columnCount : int) -> np.ndarray:\n",
    "        nonlocal start_index\n",
    "        end_index = start_index + columnCount\n",
    "        outVar = arr[start_index: end_index]\n",
    "        start_index = end_index\n",
    "        return outVar\n",
    "\n",
    "    df = pd.read_csv(filePath)\n",
    "    arr = df.values\n",
    "    \n",
    "    S = getNextColumns(STATE_FEATURES)\n",
    "    A = getNextColumns(ACTION_OPTIONS)\n",
    "    R = getNextColumns(1)\n",
    "    S_Prime = getNextColumns(STATE_FEATURES)\n",
    "    \n",
    "    return ReplayBuffer(S,A,R,S_Prime)\n",
    "\n",
    "\n",
    "def SaveReplayBuffer(replayBuffer : ReplayBuffer, filePath : str):\n",
    "    start_index = 0\n",
    "    csvArray = np.ndarray((replayBuffer.Rows,TOTAL_COLUMNS))\n",
    "    def setNextColumns(arr):\n",
    "        arr = nnu.convertToNumpy(arr)\n",
    "        nonlocal start_index, csvArray\n",
    "        end_index = start_index + arr.shape[1]\n",
    "        csvArray[:,start_index:end_index] = arr\n",
    "        start_index = end_index\n",
    "    \n",
    "    setNextColumns(replayBuffer.S)\n",
    "    setNextColumns(replayBuffer.A)\n",
    "    setNextColumns(replayBuffer.R)\n",
    "    setNextColumns(replayBuffer.S_Prime)\n",
    "\n",
    "    column_names = [\"x\", \"ẋ\", \"y\", \"ẏ\", \"θ\", \"dθ/dt\", \"L_touching\", \"R_touching\", \"Do Nothing\", \"Left Thruster\", \"Right Thruster\", \"Main Thruster\",\"Reward\",\"x'\", \"ẋ'\", \"y'\", \"ẏ'\", \"θ'\", \"dθ/dt'\", \"L_touching'\", \"R_touching'\"]\n",
    "    df = pd.DataFrame(csvArray,columns=column_names)\n",
    "    df.to_csv(filePath,  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the reward function - a crucial item to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reward(state : EnvironmentState) -> float:\n",
    "    return state.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Collect a replay buffer (s,a,R,s'). Because this is ground-truth data (independent of model decisions / inference) we can use replay data regardless of how good the model was at the time the data was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_SIZE = 10000\n",
    "\n",
    "\n",
    "def CollectData(obj) -> ReplayBuffer:\n",
    "    #input a string? load data from file    \n",
    "    if isinstance(obj, str):         \n",
    "        return LoadReplayBuffer(obj)\n",
    "    if isinstance(obj, NeuralNetwork):\n",
    "        network = obj\n",
    "        return CollectDataFromModel(network,DATA_SET_SIZE)\n",
    "\n",
    "def CollectDataFromModel(policyNetwork : NeuralNetwork, batchSize : int) -> ReplayBuffer:\n",
    "    GAME_NAME = 'LunarLander-v3'\n",
    "    env = gym.make(GAME_NAME, render_mode=None)\n",
    "    state = EnvironmentState(env.reset())\n",
    "\n",
    "    def initializeArray(rows : int, cols : int) -> np.ndarray:\n",
    "        return np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "    S = initializeArray(batchSize,STATE_FEATURES)\n",
    "    A = initializeArray(batchSize,ACTION_OPTIONS)\n",
    "    R = initializeArray(batchSize,1)\n",
    "    S_Prime = initializeArray(batchSize,STATE_FEATURES)\n",
    "\n",
    "    gamesPlayed = 0\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        stateArray = state.toArray()\n",
    "        S[i,:] = stateArray  # Update S for this row\n",
    "        R[i,0] = Reward(state) # Update R for this row\n",
    "\n",
    "        if(state.isDone):\n",
    "            gamesPlayed += 1\n",
    "            state = EnvironmentState(env.reset())\n",
    "            continue\n",
    "        \n",
    "        stateTensor = nnu.convertToTensor(stateArray)\n",
    "        q = nnu.convertToNumpy(policyNetwork.forward(stateTensor))\n",
    "        decision = np.argmax(q)\n",
    "        a = np.zeros((1,ACTION_OPTIONS))\n",
    "        a[0,decision] = 1\n",
    "        A[i,:] = a # Update A for this row\n",
    "        state = EnvironmentState(env.step(decision))\n",
    "        S_Prime[i,:] = state.toArray() # Update S_Prime for this row\n",
    "        nnu.printLoadBar(i/batchSize)\n",
    "    nnu.printLoadBar(1)\n",
    "    print(F\"\\nPlayed {gamesPlayed} full games\")\n",
    "    return ReplayBuffer(S, A, R, S_Prime)\n",
    "\n",
    "#model architecture\n",
    "\n",
    "hiddenLayers = [1000,1000,1000,4]\n",
    "activationFunctions = [nn.ReLU(), nn.ReLU(), nn.ReLU(),None]\n",
    "policy_network = NeuralNetwork(STATE_FEATURES, hiddenLayers, activationFunctions)\n",
    "target_network = policy_network.copy()\n",
    "trainingParams = nnu.TrainingParams() \n",
    "trainingParams.learningRate = 1e-2 # learning rate of the policy network during training steps\n",
    "trainingParams.regularizationConstant = 1e-1 # regularization constant used to train the policy network\n",
    "trainingParams.iterations = 60 # iterations of training when we train the policy network on any given subset of the \n",
    "loss = nn.CrossEntropyLoss()\n",
    "gamma = 0.99 #time decay of reward\n",
    "\n",
    "\n",
    "replayBufferName = \"lunarlanderReplayBuffer.csv\"\n",
    "\n",
    "generateNewData = False\n",
    "dataCollectionTool = policy_network\n",
    "if not generateNewData:\n",
    "    dataCollectionTool = replayBufferName\n",
    "\n",
    "data = CollectData(replayBufferName)\n",
    "samplingRatio = 0.1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generateNewData:\n",
    "    SaveReplayBuffer(data,replayBufferName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's run inference on the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#S, A\u001b[39;00m\n\u001b[0;32m      2\u001b[0m q_s_prime \u001b[38;5;241m=\u001b[39m target_network\u001b[38;5;241m.\u001b[39mforward(X) \u001b[38;5;66;03m# n x q tensor where n = number of rows and q = number of action possibilities\u001b[39;00m\n\u001b[0;32m      3\u001b[0m Y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mR \u001b[38;5;241m+\u001b[39m q_s_prime \u001b[38;5;241m*\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Dev\\LunarLander\\NeuralNetUtils.py:290\u001b[0m, in \u001b[0;36mReplayBuffer.generate_X\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_X\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "X = data.generate_X() #S, A\n",
    "q_s_prime = target_network.forward(X) # n x q tensor where n = number of rows and q = number of action possibilities\n",
    "Y = data.R + q_s_prime * gamma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
