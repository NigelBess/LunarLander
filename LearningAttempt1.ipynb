{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to collect data for training a DQN to land the lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import NeuralNetUtils as nnu\n",
    "from NeuralNetUtils import NeuralNetwork, ReplayBuffer\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers specific to the lunar lander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FEATURES = 8 # x, ẋ, y, ẏ, θ, θ*, rightlegtouching, leftlegtouching\n",
    "ACTION_OPTIONS = 4 # do nothing, fire left, fire main, fire right\n",
    "TOTAL_COLUMNS = STATE_FEATURES * 2 + ACTION_OPTIONS + 1 # S + A + R + S = 2*S + A + 1\n",
    "\n",
    "\n",
    "#converts env state space into a more easily useable object\n",
    "class EnvironmentState:\n",
    "    def __init__(self,state):\n",
    "        coord = state[0]\n",
    "        self.x = coord[0]\n",
    "        self.y = coord[1]\n",
    "        self.x_dot = coord[2]\n",
    "        self.y_dot = coord[3]\n",
    "        self.theta = coord[4]\n",
    "        self.theta_dot = coord[5]\n",
    "        self.leftLegTouching = int(coord[6])\n",
    "        self.rightLegTouching = int(coord[7])\n",
    "        reward = 0\n",
    "        isDone = False\n",
    "        if len(state)>2:\n",
    "            reward = state[1]\n",
    "            isDone = state[2] or state[3]\n",
    "        self.reward = reward\n",
    "        self.isDone = isDone \n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\n",
    "            f\"(x, y): ({self.x}, {self.y})\",\n",
    "            f\"(ẋ, ẏ): ({self.x_dot}, {self.y_dot})\",\n",
    "            f\"θ: {self.theta}\",\n",
    "            f\"dθ/dt: {self.theta_dot}\",\n",
    "            f\"Legs touching (L,R): ({self.leftLegTouching}, {self.rightLegTouching})\",\n",
    "            f\" Reward: {self.reward}\",\n",
    "            f\" Done: {self.isDone}\",\n",
    "        ]\n",
    "        return \"\\n\".join(lines,)\n",
    "\n",
    "    def toTensor(self):\n",
    "        return nnu.convertToTensor(self.toArray().astype(np.float32))\n",
    "    \n",
    "    def toArray(self) -> np.ndarray:\n",
    "        return np.array(self.toList())\n",
    "    \n",
    "    def toList(self) -> List[float]:\n",
    "        return [self.x, self.y, self.x_dot, self.y_dot, self.theta, self.theta_dot, self.leftLegTouching, self.rightLegTouching]\n",
    "    \n",
    "# reads file to load a replay buffer\n",
    "def LoadReplayBuffer(filePath : str) -> ReplayBuffer:\n",
    "\n",
    "    start_index = 0\n",
    "    def getNextColumns(columnCount : int) -> np.ndarray:\n",
    "        nonlocal start_index\n",
    "        end_index = start_index + columnCount\n",
    "        outVar = arr[start_index: end_index]\n",
    "        start_index = end_index\n",
    "        return outVar\n",
    "\n",
    "    df = pd.read_csv(filePath)\n",
    "    arr = df.values\n",
    "    \n",
    "    S = getNextColumns(STATE_FEATURES)\n",
    "    A = getNextColumns(ACTION_OPTIONS)\n",
    "    R = getNextColumns(1)\n",
    "    S_Prime = getNextColumns(STATE_FEATURES)\n",
    "    \n",
    "    return ReplayBuffer(S,A,R,S_Prime)\n",
    "\n",
    "\n",
    "def SaveReplayBuffer(replayBuffer : ReplayBuffer, filePath : str):\n",
    "    start_index = 0\n",
    "    csvArray = np.ndarray((replayBuffer.Rows,TOTAL_COLUMNS))\n",
    "    def setNextColumns(arr : np.ndarray):\n",
    "        nonlocal start_index, csvArray\n",
    "        end_index = start_index + arr.shape[1]\n",
    "        csvArray[:,start_index:end_index] = arr\n",
    "        start_index = end_index\n",
    "    \n",
    "    setNextColumns(replayBuffer.S)\n",
    "    setNextColumns(replayBuffer.A)\n",
    "    setNextColumns(replayBuffer.R)\n",
    "    setNextColumns(replayBuffer.S_Prime)\n",
    "\n",
    "    column_names = [\"x\", \"ẋ\", \"y\", \"ẏ\", \"θ\", \"dθ/dt\", \"L_touching\", \"R_touching\", \"Do Nothing\", \"Left Thruster\", \"Right Thruster\", \"Main Thruster\",\"Reward\",\"x'\", \"ẋ'\", \"y'\", \"ẏ'\", \"θ'\", \"dθ/dt'\", \"L_touching'\", \"R_touching'\"]\n",
    "    df = pd.DataFrame(csvArray,columns=column_names)\n",
    "    df.to_csv(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the reward function - a crucial item to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reward(state : EnvironmentState) -> float:\n",
    "    return state.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Collect a replay buffer (s,a,R,s'). Because this is ground-truth data (independent of model decisions / inference) we can use replay data regardless of how good the model was at the time the data was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_SIZE = 100000\n",
    "\n",
    "\n",
    "def CollectData(obj) -> ReplayBuffer:\n",
    "    #input a string? load data from file    \n",
    "    if obj is str:        \n",
    "        return LoadReplayBuffer(obj)\n",
    "    if obj is NeuralNetwork:\n",
    "        network = obj\n",
    "        return CollectDataFromModel(network,DATA_SET_SIZE)\n",
    "\n",
    "def CollectDataFromModel(policyNetwork : NeuralNetwork, batchSize : int) -> ReplayBuffer:\n",
    "    GAME_NAME = 'LunarLander-v3'\n",
    "    env = gym.make(GAME_NAME, render_mode=None)\n",
    "    state = EnvironmentState(env.reset())\n",
    "\n",
    "    def initializeArray(rows : int, cols : int) -> np.ndarray:\n",
    "        return np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "    S = initializeArray(batchSize,STATE_FEATURES)\n",
    "    A = initializeArray(batchSize,ACTION_OPTIONS)\n",
    "    R = initializeArray(batchSize,1)\n",
    "    S_Prime = initializeArray(batchSize,STATE_FEATURES)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        stateArray = state.toArray()\n",
    "        S[i,:] = stateArray  # Update S for this row\n",
    "        R[i,0] = Reward(state) # Update R for this row\n",
    "\n",
    "        if(state.isDone):\n",
    "            state = env.reset()\n",
    "            continue\n",
    "        \n",
    "        stateTensor = nnu.convertToTensor(stateArray)\n",
    "        q = nnu.convertToNumpy(policyNetwork.forward(stateTensor))\n",
    "        decision = np.argmax(q)\n",
    "        a = np.zeros(1,ACTION_OPTIONS)\n",
    "        a[decision] = 1\n",
    "        A[i,:] = a # Update A for this row\n",
    "        state = EnvironmentState(env.step(decision))\n",
    "        S_Prime[i,:] = state.toArray() # Update S_Prime for this row\n",
    "    \n",
    "    return ReplayBuffer(S, A, R, S_Prime)\n",
    "\n",
    "#model architecture\n",
    "\n",
    "hiddenLayers = [1000,1000,1000,4]\n",
    "layers = nnu.buildLayers(hiddenLayers)\n",
    "policy_network = NeuralNetwork(STATE_FEATURES,layers)\n",
    "target_network = policy_network.copy()\n",
    "trainingParams = nnu.TrainingParams() \n",
    "trainingParams.learningRate = 1e-2\n",
    "trainingParams.regularizationConstant = 1e-1\n",
    "trainingParams.iterations = 60\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
